{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab07a3-6ae4-4075-a2bc-74974e234ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_markets():\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import time\n",
    "    import os\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    from tqdm.notebook import tqdm\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    from rapidfuzz import fuzz\n",
    "\n",
    "    # Replace with actual file locations\n",
    "    match_files = [\n",
    "        r\"Kalshi_PredictIt_Matches.csv\",\n",
    "        r\"Kalshi_Polymarket_Matches.csv\",\n",
    "        r\"PredictIt_Polymarket_Matches.csv\"\n",
    "    ]\n",
    "    \n",
    "    for match_file in match_files:\n",
    "        old_match_file = match_file.replace(\".csv\", \"_Old.csv\")\n",
    "        if os.path.exists(match_file):\n",
    "            try:\n",
    "                if os.path.exists(old_match_file):\n",
    "                    os.remove(old_match_file)  # Remove existing file\n",
    "                os.rename(match_file, old_match_file)\n",
    "            except FileExistsError:\n",
    "                print(f\"Could not rename {match_file} to {old_match_file} as it still exists\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error renaming {match_file}: {str(e)}\")\n",
    "    \n",
    "    def preprocess_name(name):\n",
    "        if pd.isna(name):\n",
    "            return \"\"\n",
    "        name = str(name).lower()\n",
    "        # Synonym mapping\n",
    "        synonyms = {\n",
    "            'gubernatorial': 'governorship',\n",
    "            'presidential': 'president',\n",
    "            'senatorial': 'senate'\n",
    "        }\n",
    "        # State abbreviation mapping for all 50 states and DC\n",
    "        state_abbrevs = [\n",
    "            'al-', 'ak-', 'az-', 'ar-', 'ca-', 'co-', 'ct-', 'de-', 'fl-', 'ga-',\n",
    "            'hi-', 'id-', 'il-', 'in-', 'ia-', 'ks-', 'ky-', 'la-', 'me-', 'md-',\n",
    "            'ma-', 'mi-', 'mn-', 'ms-', 'mo-', 'mt-', 'ne-', 'nv-', 'nh-', 'nj-',\n",
    "            'nm-', 'ny-', 'nc-', 'nd-', 'oh-', 'ok-', 'or-', 'pa-', 'ri-', 'sc-',\n",
    "            'sd-', 'tn-', 'tx-', 'ut-', 'vt-', 'va-', 'wa-', 'wv-', 'wi-', 'wy-',\n",
    "            'dc-'\n",
    "        ]\n",
    "        state_names = [\n",
    "            'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida', 'georgia',\n",
    "            'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine', 'maryland',\n",
    "            'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska', 'nevada', 'new hampshire', 'new jersey',\n",
    "            'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina',\n",
    "            'south dakota', 'tennessee', 'texas', 'utah', 'vermont', 'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming',\n",
    "            'district of columbia'\n",
    "        ]\n",
    "        state_dict = {abbrev[:-1]: name for abbrev, name in zip(state_abbrevs, state_names)}\n",
    "        for abbrev, full in state_dict.items():\n",
    "            name = re.sub(r'\\b' + re.escape(abbrev) + r'-(\\d+)\\b',\n",
    "                          lambda m: f\"{full} {m.group(1)} district\", name)\n",
    "        name = re.sub(r'\\b(\\d+)(st|nd|rd|th)\\b', r'\\1', name)\n",
    "        for old, new in synonyms.items():\n",
    "            name = re.sub(r'\\b' + old + r'\\b', new, name)\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        name = re.sub(r'district', 'district ', name)\n",
    "        return name\n",
    "    \n",
    "    def extract_timeframe(text, end_date):\n",
    "        timeframes = []\n",
    "        text = str(text).lower()\n",
    "        # Month mapping for normalization\n",
    "        month_map = {\n",
    "            'jan': 'january', 'feb': 'february', 'mar': 'march', 'apr': 'april',\n",
    "            'may': 'may', 'jun': 'june', 'jul': 'july', 'aug': 'august',\n",
    "            'sep': 'september', 'oct': 'october', 'nov': 'november', 'dec': 'december'\n",
    "        }\n",
    "        # Extract years\n",
    "        years = re.findall(r'\\b(20\\d{2})\\b', text)\n",
    "        timeframes.extend(years)\n",
    "        specific_dates = re.findall(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\s+\\d{1,2}\\s*,\\s*(20\\d{2})', text)\n",
    "        for month, year in specific_dates:\n",
    "            timeframes.append(f\"{month_map[month.lower()]} {year}\")\n",
    "        standalone_months = re.findall(r'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b', text)\n",
    "        for month in standalone_months:\n",
    "            timeframes.append(month_map[month.lower()])\n",
    "        before_years = re.findall(r'\\bbefore\\s+(20\\d{2})\\b', text)\n",
    "        for year in before_years:\n",
    "            timeframes.append(year)\n",
    "            try:\n",
    "                prev_year = str(int(year) - 1)\n",
    "                timeframes.append(prev_year)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        relative_terms = [\"this month\", \"this year\", \"before august\"] if \"before august\" in text else []\n",
    "        timeframes.extend(relative_terms)\n",
    "        # Extract year from end_date\n",
    "        if isinstance(end_date, str) and not pd.isna(end_date):\n",
    "            try:\n",
    "                end_date_year = datetime.strptime(end_date, \"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "                timeframes.append(str(end_date_year))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        return list(set(timeframes))\n",
    "    \n",
    "    def extract_entities(text):\n",
    "        text = str(text)\n",
    "        # State abbreviation mapping\n",
    "        state_abbrevs = {\n",
    "            'al': 'alabama', 'ak': 'alaska', 'az': 'arizona', 'ar': 'arkansas', 'ca': 'california',\n",
    "            'co': 'colorado', 'ct': 'connecticut', 'de': 'delaware', 'fl': 'florida', 'ga': 'georgia',\n",
    "            'hi': 'hawaii', 'id': 'idaho', 'il': 'illinois', 'in': 'indiana', 'ia': 'iowa',\n",
    "            'ks': 'kansas', 'ky': 'kentucky', 'la': 'louisiana', 'me': 'maine', 'md': 'maryland',\n",
    "            'ma': 'massachusetts', 'mi': 'michigan', 'mn': 'minnesota', 'ms': 'mississippi', 'mo': 'missouri',\n",
    "            'mt': 'montana', 'ne': 'nebraska', 'nv': 'nevada', 'nh': 'new hampshire', 'nj': 'new jersey',\n",
    "            'nm': 'new mexico', 'ny': 'new york', 'nc': 'north carolina', 'nd': 'north dakota', 'oh': 'ohio',\n",
    "            'ok': 'oklahoma', 'or': 'oregon', 'pa': 'pennsylvania', 'ri': 'rhode island', 'sc': 'south carolina',\n",
    "            'sd': 'south dakota', 'tn': 'tennessee', 'tx': 'texas', 'ut': 'utah', 'vt': 'vermont',\n",
    "            'va': 'virginia', 'wa': 'washington', 'wv': 'west virginia', 'wi': 'wisconsin', 'wy': 'wyoming',\n",
    "            'dc': 'district of columbia'\n",
    "        }\n",
    "        pattern = r'\\b(?:[A-Z][a-z]*\\s+)+[A-Z][a-z]*\\b'\n",
    "        entities = re.findall(pattern, text)\n",
    "        # Normalize state-district pairs\n",
    "        district_pattern = r'\\b([A-Z][a-z\\s]*(?:\\'s)? \\d+(?:st|nd|rd|th)? District\\b|\\b[A-Z]{2}-\\d+\\b)'\n",
    "        districts = re.findall(district_pattern, text)\n",
    "        normalized_districts = []\n",
    "        for district in districts:\n",
    "            for abbrev, full in state_abbrevs.items():\n",
    "                if abbrev.upper() in district:\n",
    "                    district = district.replace(abbrev.upper(), full.title())\n",
    "                elif district.startswith(full.title()):\n",
    "                    district = district.replace(full.title(), full.title())\n",
    "            district = re.sub(r'\\b(\\d+)(st|nd|rd|th)\\b', r'\\1', district)\n",
    "            normalized_districts.append(district)\n",
    "        entities.extend(normalized_districts)\n",
    "        # Include specific single-word entities and all state names\n",
    "        known_entities = {\n",
    "            'US', 'Trump', 'Ripple', 'Truth Social', 'The Witcher', 'Rotten Tomatoes',\n",
    "            'Republican', 'Democratic', 'Run', 'Run for', 'Win', 'Winner', 'Independent', 'Governor',\n",
    "            'Gubernatorial', 'Governorship', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
    "            'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois',\n",
    "            'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts',\n",
    "            'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n",
    "            'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota',\n",
    "            'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
    "            'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin',\n",
    "            'Wyoming', 'District of Columbia', 'Germany', 'France', 'Japan', 'Canada', 'Italy', 'Palestine',\n",
    "            'Netherlands', 'Adrienne Adams', 'MVP', 'Breanna Stewart', 'Philadelphia', 'NL East', 'NL', 'Tariff',\n",
    "            'Court'\n",
    "        }\n",
    "        entities.extend([word for word in text.split() if word in known_entities])\n",
    "        numerical_patterns = [\n",
    "            r'\\b\\d+\\b(?!.*\\b20\\d{2}\\b)',  \n",
    "            r'\\b\\d+-\\d+\\b', \n",
    "            r'\\b\\d+k\\b',\n",
    "            r'\\babove \\d+\\b', \n",
    "            r'\\bbelow \\d+\\b' \n",
    "        ]\n",
    "        for pattern in numerical_patterns:\n",
    "            matches = re.findall(pattern, text.lower())\n",
    "            entities.extend(matches)\n",
    "        entities = [e.replace('1k', '1000') if '1k' in e else e for e in entities]\n",
    "        stop_words = {\n",
    "            'Will', 'Who', 'Before', 'The', 'This', 'By', 'Be', 'On', 'At', 'Season',\n",
    "            'Cards', 'Day', 'How', 'Gold', 'Year', 'Above', 'Below', 'Score', 'Party'\n",
    "        }\n",
    "        entities = [e.strip() for e in entities if e.strip() not in stop_words and len(e.strip()) > 2 and ':' not in e]\n",
    "        for entity in known_entities:\n",
    "            if entity.lower() in text.lower():\n",
    "                entities.append(entity)\n",
    "        return list(set(entities))\n",
    "    \n",
    "    def extract_district(text):\n",
    "        text = str(text).lower()\n",
    "        match = re.search(r'(?:district|dist\\.?)\\s*(\\d+)|(\\d+)(?:st|nd|rd|th)?\\s*district|\\b[a-z]{2}-(\\d+)\\b', text)\n",
    "        if match:\n",
    "            return match.group(1) or match.group(2) or match.group(3)\n",
    "        return None\n",
    "    \n",
    "    def same_district(name1, name2):\n",
    "        d1 = extract_district(name1)\n",
    "        d2 = extract_district(name2)\n",
    "        return d1 == d2  # True if same district number or both None\n",
    "    \n",
    "    def are_timeframes_compatible(tf1, tf2):\n",
    "        if not tf1 or not tf2:\n",
    "            return True\n",
    "        for t1 in tf1:\n",
    "            for t2 in tf2:\n",
    "                if t1 == t2:\n",
    "                    return True\n",
    "                if t1.startswith(\"20\") and t2.split()[-1] == t1:\n",
    "                    return True\n",
    "                if t2.startswith(\"20\") and t1.split()[-1] == t2:\n",
    "                    return True\n",
    "                if (t1 == \"this year\" or t1 == \"before august\") and (t2.startswith(\"20\") or t2.split()[-1].startswith(\"20\")):\n",
    "                    return True\n",
    "                if (t2 == \"this year\" or t2 == \"before august\") and (t1.startswith(\"20\") or t1.split()[-1].startswith(\"20\")):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    # Check entity overlap\n",
    "    def entities_overlap(entities1, entities2):\n",
    "        if not entities1 or not entities2:\n",
    "            return True\n",
    "        # Define critical action words that must match or be absent\n",
    "        action_words = {'run', 'win', 'nominate', 'elect', 'appointed', 'confirmed'}\n",
    "        # Define party entities that must match or be absent\n",
    "        party_words = {'democratic', 'republican', 'independent'}\n",
    "        # Define district entities\n",
    "        district_words = {e for e in entities1 if 'District' in e or '-' in e} | {e for e in entities2 if 'District' in e or '-' in e}\n",
    "        entities1_actions = [e.lower() for e in entities1 if e.lower() in action_words]\n",
    "        entities2_actions = [e.lower() for e in entities2 if e.lower() in action_words]\n",
    "        entities1_parties = [e.lower() for e in entities1 if e.lower() in party_words]\n",
    "        entities2_parties = [e.lower() for e in entities2 if e.lower() in party_words]\n",
    "        entities1_districts = [e for e in entities1 if 'District' in e or '-' in e]\n",
    "        entities2_districts = [e for e in entities2 if 'District' in e or '-' in e]\n",
    "        # If either has an action word, they must match exactly\n",
    "        if entities1_actions or entities2_actions:\n",
    "            if set(entities1_actions) != set(entities2_actions):\n",
    "                return False\n",
    "        # If either has a party, they must match exactly\n",
    "        if entities1_parties or entities2_parties:\n",
    "            if set(entities1_parties) != set(entities2_parties):\n",
    "                return False\n",
    "        # If either has a district, they must match exactly\n",
    "        if entities1_districts or entities2_districts:\n",
    "            if set(entities1_districts) != set(entities2_districts):\n",
    "                return False\n",
    "        # Require matching state if both have one\n",
    "        us_states = {\n",
    "            'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut',\n",
    "            'delaware', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa',\n",
    "            'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan',\n",
    "            'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska', 'nevada',\n",
    "            'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina',\n",
    "            'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island',\n",
    "            'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'vermont',\n",
    "            'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming', 'district of columbia'\n",
    "        }\n",
    "        states1 = {e.lower() for e in entities1 if e.lower() in us_states}\n",
    "        states2 = {e.lower() for e in entities2 if e.lower() in us_states}\n",
    "        if states1 and states2:\n",
    "            if states1 != states2:\n",
    "                return False\n",
    "        # Require matching country if both have one\n",
    "        countries = {\n",
    "            'germany', 'france', 'japan', 'canada', 'italy', 'israel', 'uk', 'spain', 'us', 'u.s.', 'palestine', 'netherlands'\n",
    "        }\n",
    "        countries1 = {e.lower() for e in entities1 if e.lower() in countries}\n",
    "        countries2 = {e.lower() for e in entities2 if e.lower() in countries}\n",
    "        if countries1 and countries2:\n",
    "            if countries1 != countries2:\n",
    "                return False\n",
    "        # Require at least one shared non-action, non-party, non-district entity\n",
    "        non_action_party_district_entities1 = [e for e in entities1 if e.lower() not in action_words and e.lower() not in party_words and e not in district_words]\n",
    "        non_action_party_district_entities2 = [e for e in entities2 if e.lower() not in action_words and e.lower() not in party_words and e not in district_words]\n",
    "        if not non_action_party_district_entities1 or not non_action_party_district_entities2:\n",
    "            return True\n",
    "        return any(e1.lower() in [e2.lower() for e2 in non_action_party_district_entities2] for e1 in non_action_party_district_entities1)\n",
    "    \n",
    "    kalshi_data = pd.read_csv(r\"Kalshi.csv\") # replace with actual file location\n",
    "    polymarket_data = pd.read_csv(r\"Polymarket.csv\") # replace with actual file location\n",
    "    \n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"Above\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bAbove', 'Greater than', regex=True)\n",
    "    polymarket_data.loc[polymarket_data['title'].str.contains(\"WNBA MVP\", na=False), 'title'] = \\\n",
    "        polymarket_data['title'].str.replace(r'\\bWNBA MVP', 'MVP', regex=True)\n",
    "    polymarket_data.loc[polymarket_data['title'].str.contains(\"National League\", na=False), 'title'] = \\\n",
    "        polymarket_data['title'].str.replace(r'\\bNational League', 'NL', regex=True)\n",
    "    \n",
    "    # Pivot data for prices\n",
    "    kalshi_pivot = kalshi_data.pivot_table(\n",
    "        index=[\"full_name\", \"market_id\", \"end_date\"],\n",
    "        columns=\"side\",\n",
    "        values=\"ask\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "    kalshi_pivot.columns.name = None\n",
    "    kalshi_pivot.rename(columns={\"YES\": \"kalshi_yes\", \"NO\": \"kalshi_no\"}, inplace=True)\n",
    "    \n",
    "    polymarket_pivot = polymarket_data.pivot_table(\n",
    "        index=[\"title\", \"event_id\", \"end_date\"],\n",
    "        columns=\"outcome\",\n",
    "        values=\"price\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "    polymarket_pivot.columns.name = None\n",
    "    polymarket_pivot.rename(columns={\"Yes\": \"polymarket_yes\", \"No\": \"polymarket_no\"}, inplace=True)\n",
    "    \n",
    "    # Preprocess event names and extract timeframes\n",
    "    kalshi_pivot['processed_name'] = kalshi_pivot['full_name'].apply(preprocess_name)\n",
    "    polymarket_pivot['processed_name'] = polymarket_pivot['title'].apply(preprocess_name)\n",
    "    kalshi_pivot['timeframes'] = kalshi_pivot.apply(lambda x: extract_timeframe(x['full_name'], x['end_date']), axis=1)\n",
    "    polymarket_pivot['timeframes'] = polymarket_pivot.apply(lambda x: extract_timeframe(x['title'], x['end_date']), axis=1)\n",
    "    kalshi_pivot['entities'] = kalshi_pivot['full_name'].apply(extract_entities)\n",
    "    polymarket_pivot['entities'] = polymarket_pivot['title'].apply(extract_entities)\n",
    "    \n",
    "    # Filter by year\n",
    "    kalshi_years = kalshi_pivot['timeframes'].apply(lambda x: set(x).intersection({'2025', 'this year', 'before august'}))\n",
    "    polymarket_years = polymarket_pivot['timeframes'].apply(lambda x: set(x).intersection({'2025', 'this year', 'before august'}))\n",
    "    kalshi_subset = kalshi_pivot[kalshi_years.apply(len) > 0]\n",
    "    polymarket_subset = polymarket_pivot[polymarket_years.apply(len) > 0]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    kalshi_vectors = vectorizer.fit_transform(kalshi_subset['processed_name'])\n",
    "    polymarket_vectors = vectorizer.transform(polymarket_subset['processed_name'])\n",
    "    cosine_sim = cosine_similarity(kalshi_vectors, polymarket_vectors)\n",
    "    \n",
    "    # Filter pairs with high similarity\n",
    "    cosine_threshold = 0.8\n",
    "    similar_pairs = np.where(cosine_sim >= cosine_threshold)\n",
    "    matches = []\n",
    "    \n",
    "    # Perform precise matching with rapidfuzz\n",
    "    for idx in tqdm(range(len(similar_pairs[0])), desc=\"Matching similar pairs\"):\n",
    "        i, j = similar_pairs[0][idx], similar_pairs[1][idx]\n",
    "        name1 = kalshi_subset['processed_name'].iloc[i]\n",
    "        name2 = polymarket_subset['processed_name'].iloc[j]\n",
    "        tf1 = kalshi_subset['timeframes'].iloc[i]\n",
    "        tf2 = polymarket_subset['timeframes'].iloc[j]\n",
    "        ent1 = kalshi_subset['entities'].iloc[i]\n",
    "        ent2 = polymarket_subset['entities'].iloc[j]\n",
    "        kalshi_event = kalshi_subset['full_name'].iloc[i]\n",
    "        polymarket_event = polymarket_subset['title'].iloc[j]\n",
    "        if name1 and name2 and same_district(name1, name2):\n",
    "            similarity = fuzz.token_sort_ratio(name1, name2)\n",
    "            timeframe_ok = are_timeframes_compatible(tf1, tf2)\n",
    "            entity_ok = entities_overlap(ent1, ent2)\n",
    "            threshold = 81 if any(x in kalshi_event.lower() or x in polymarket_event.lower() for x in [\"how many\", \"less than\", \"between\", \">\", \"<\"]) else 70\n",
    "            if similarity >= threshold and timeframe_ok and entity_ok:\n",
    "                kalshi_yes = kalshi_subset['kalshi_yes'].iloc[i]\n",
    "                kalshi_no = kalshi_subset['kalshi_no'].iloc[i]\n",
    "                polymarket_yes = polymarket_subset['polymarket_yes'].iloc[j] * 100 if pd.notna(polymarket_subset['polymarket_yes'].iloc[j]) else None\n",
    "                polymarket_no = polymarket_subset['polymarket_no'].iloc[j] * 100 if pd.notna(polymarket_subset['polymarket_no'].iloc[j]) else None\n",
    "                # Calculate arbitrage\n",
    "                calc_kalshi_yes = kalshi_yes if pd.notna(kalshi_yes) else (100 - kalshi_no if pd.notna(kalshi_no) else 0)\n",
    "                calc_kalshi_no = kalshi_no if pd.notna(kalshi_no) else (100 - kalshi_yes if pd.notna(kalshi_yes) else 0)\n",
    "                calc_polymarket_yes = polymarket_yes if pd.notna(polymarket_yes) else (100 - polymarket_no if pd.notna(polymarket_no) else 0)\n",
    "                calc_polymarket_no = polymarket_no if pd.notna(polymarket_no) else (100 - polymarket_yes if pd.notna(polymarket_yes) else 0)\n",
    "                matches.append({\n",
    "                    'Kalshi_Event': kalshi_event,\n",
    "                    'Polymarket_Event': polymarket_event,\n",
    "                    'Similarity_Score': similarity,\n",
    "                    'Timeframes_Match': timeframe_ok,\n",
    "                    'Entities_Match': entity_ok,\n",
    "                    'Kalshi_Timeframes': tf1,\n",
    "                    'Polymarket_Timeframes': tf2,\n",
    "                    'Kalshi_Entities': ent1,\n",
    "                    'Polymarket_Entities': ent2,\n",
    "                    'kalshi_yes': kalshi_yes,\n",
    "                    'kalshi_no': kalshi_no,\n",
    "                    'polymarket_yes': polymarket_yes,\n",
    "                    'polymarket_no': polymarket_no,\n",
    "                    'arb1': 100 - calc_polymarket_yes - calc_kalshi_no,\n",
    "                    'arb2': 100 - calc_polymarket_no - calc_kalshi_yes\n",
    "                })\n",
    "    \n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    if not matches_df.empty:\n",
    "        matches_df = matches_df.sort_values(by='Similarity_Score', ascending=False)\n",
    "        matches_df = matches_df.drop_duplicates(subset=['Kalshi_Event', 'Polymarket_Event'], keep='first')\n",
    "    \n",
    "    # Filter for significant arbitrage and non-zero prices\n",
    "    matches_df = matches_df[\n",
    "        ((matches_df[\"arb1\"] >= 10) | (matches_df[\"arb2\"] >= 10)) &\n",
    "        (matches_df[\"kalshi_yes\"].notna() & (matches_df[\"kalshi_yes\"] != 0)) &\n",
    "        (matches_df[\"kalshi_no\"].notna() & (matches_df[\"kalshi_no\"] != 0)) &\n",
    "        (matches_df[\"polymarket_yes\"].notna() & (matches_df[\"polymarket_yes\"] != 0)) &\n",
    "        (matches_df[\"polymarket_no\"].notna() & (matches_df[\"polymarket_no\"] != 0))\n",
    "    ]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = r\"Kalshi_Polymarket_Matches.csv\" # replace with desired file location\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(matches_df)} matched markets\")\n",
    "    def preprocess_name(name):\n",
    "        if pd.isna(name):\n",
    "            return \"\"\n",
    "        name = str(name).lower()\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name\n",
    "    \n",
    "    def extract_timeframe(text, end_date):\n",
    "        timeframes = []\n",
    "        text = str(text).lower()\n",
    "        years = re.findall(r'\\b(20\\d{2})\\b', text)\n",
    "        specific_dates = re.findall(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\s+\\d{1,2}\\s*,\\s*(20\\d{2})', text)\n",
    "        relative_terms = [\"this month\", \"this year\", \"before august\"] if \"before august\" in text else []\n",
    "        timeframes.extend(years)\n",
    "        timeframes.extend([date[0] for date in specific_dates])\n",
    "        timeframes.extend(relative_terms)\n",
    "        if isinstance(end_date, str) and not pd.isna(end_date):\n",
    "            try:\n",
    "                end_date_year = datetime.strptime(end_date, \"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "                timeframes.append(str(end_date_year))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        return list(set(timeframes))\n",
    "    \n",
    "    def extract_entities(text):\n",
    "        text = str(text)\n",
    "        pattern = r'\\b(?:[A-Z][a-z]*\\s+)+[A-Z][a-z]*\\b'\n",
    "        entities = re.findall(pattern, text)\n",
    "        # Include specific single-word (easily expandable to improve accuracy)\n",
    "        known_entities = {'US', 'Trump', 'Ripple', 'Truth Social', 'The Witcher', 'Rotten Tomatoes'}\n",
    "        entities.extend([word for word in text.split() if word in known_entities])\n",
    "        # Extract numerical outcomes (exclude years)\n",
    "        numerical_patterns = [\n",
    "            r'\\b\\d+\\b(?!.*\\b20\\d{2}\\b)',\n",
    "            r'\\b\\d+-\\d+\\b', \n",
    "            r'\\b\\d+k\\b', \n",
    "            r'\\babove \\d+\\b',  \n",
    "            r'\\bbelow \\d+\\b' \n",
    "        ]\n",
    "        for pattern in numerical_patterns:\n",
    "            matches = re.findall(pattern, text.lower())\n",
    "            entities.extend(matches)\n",
    "        entities = [e.replace('1k', '1000') if '1k' in e else e for e in entities]\n",
    "        stop_words = {'Will', 'Who', 'Before', 'The', 'This', 'By', 'Be', 'In', 'On', 'At', 'Season', 'Cards', 'Day', 'How', 'Gold', 'Year', 'Above', 'Below', 'Score'}\n",
    "        entities = [e.strip() for e in entities if e.strip() not in stop_words and len(e.strip()) > 2 and ':' not in e]\n",
    "        # Add known entities\n",
    "        for entity in known_entities:\n",
    "            if entity.lower() in text.lower():\n",
    "                entities.append(entity)\n",
    "        return list(set(entities))\n",
    "    \n",
    "    # Check timeframe compatibility\n",
    "    def are_timeframes_compatible(tf1, tf2):\n",
    "        if not tf1 or not tf2:\n",
    "            return True\n",
    "        for t1 in tf1:\n",
    "            for t2 in tf2:\n",
    "                if t1 == t2:\n",
    "                    return True\n",
    "                if (t1 == \"this year\" or t1 == \"before august\") and t2 == \"2025\":\n",
    "                    return True\n",
    "                if (t2 == \"this year\" or t2 == \"before august\") and t1 == \"2025\":\n",
    "                    return True\n",
    "                if t1.startswith(\"20\") and t2.startswith(\"20\") and t1 == t2:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    # Check entity overlap\n",
    "    def entities_overlap(entities1, entities2):\n",
    "        if not entities1 or not entities2:\n",
    "            return True\n",
    "        return any(e1.lower() in [e2.lower() for e2 in entities2] for e1 in entities1)\n",
    "    \n",
    "    predictit_data = pd.read_csv(r\"PredictIt.csv\") # replace with actual file location\n",
    "    polymarket_data = pd.read_csv(r\"Polymarket.csv\") # replace with actual file location\n",
    "    predictit_data[\"price\"] = predictit_data[\"price\"] * 100\n",
    "    \n",
    "    # Pivot data for prices\n",
    "    polymarket_pivot = polymarket_data.pivot_table(\n",
    "        index=[\"title\", \"event_id\", \"end_date\"],\n",
    "        columns=\"outcome\",\n",
    "        values=\"price\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "    polymarket_pivot.columns.name = None\n",
    "    polymarket_pivot.rename(columns={\"Yes\": \"polymarket_yes\", \"No\": \"polymarket_no\"}, inplace=True)\n",
    "    \n",
    "    predictit_pivot = predictit_data.pivot_table(\n",
    "        index=[\"market_name\", \"market_id\", \"end_date\"],\n",
    "        columns=\"side\",\n",
    "        values=\"price\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "    predictit_pivot.columns.name = None\n",
    "    predictit_pivot.rename(columns={\"YES\": \"predictit_yes\", \"NO\": \"predictit_no\"}, inplace=True)\n",
    "    \n",
    "    # Preprocess event names and extract timeframes\n",
    "    polymarket_pivot['processed_name'] = polymarket_pivot['title'].apply(preprocess_name)\n",
    "    predictit_pivot['processed_name'] = predictit_pivot['market_name'].apply(preprocess_name)\n",
    "    polymarket_pivot['timeframes'] = polymarket_pivot.apply(lambda x: extract_timeframe(x['title'], x['end_date']), axis=1)\n",
    "    predictit_pivot['timeframes'] = predictit_pivot.apply(lambda x: extract_timeframe(x['market_name'], x['end_date']), axis=1)\n",
    "    polymarket_pivot['entities'] = polymarket_pivot['title'].apply(extract_entities)\n",
    "    predictit_pivot['entities'] = predictit_pivot['market_name'].apply(extract_entities)\n",
    "    \n",
    "    # Filter by year\n",
    "    predictit_years = predictit_pivot['timeframes'].apply(lambda x: set(x).intersection({'2025', 'this year', 'before august'}))\n",
    "    polymarket_years = polymarket_pivot['timeframes'].apply(lambda x: set(x).intersection({'2025', 'this year', 'before august'}))\n",
    "    predictit_subset = predictit_pivot[predictit_years.apply(len) > 0]\n",
    "    polymarket_subset = polymarket_pivot[polymarket_years.apply(len) > 0]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    predictit_vectors = vectorizer.fit_transform(predictit_subset['processed_name'])\n",
    "    polymarket_vectors = vectorizer.transform(polymarket_subset['processed_name'])\n",
    "    cosine_sim = cosine_similarity(predictit_vectors, polymarket_vectors)\n",
    "    \n",
    "    # Filter pairs with high similarity\n",
    "    cosine_threshold = 0.8\n",
    "    similar_pairs = np.where(cosine_sim >= cosine_threshold)\n",
    "    matches = []\n",
    "    \n",
    "    # Perform precise matching with rapidfuzz\n",
    "    for idx in tqdm(range(len(similar_pairs[0])), desc=\"Matching similar pairs\"):\n",
    "        i, j = similar_pairs[0][idx], similar_pairs[1][idx]\n",
    "        name1 = predictit_subset['processed_name'].iloc[i]\n",
    "        name2 = polymarket_subset['processed_name'].iloc[j]\n",
    "        tf1 = predictit_subset['timeframes'].iloc[i]\n",
    "        tf2 = polymarket_subset['timeframes'].iloc[j]\n",
    "        ent1 = predictit_subset['entities'].iloc[i]\n",
    "        ent2 = polymarket_subset['entities'].iloc[j]\n",
    "        predictit_event = predictit_subset['market_name'].iloc[i]\n",
    "        polymarket_event = polymarket_subset['title'].iloc[j]\n",
    "        if name1 and name2:\n",
    "            similarity = fuzz.token_sort_ratio(name1, name2)\n",
    "            timeframe_ok = are_timeframes_compatible(tf1, tf2)\n",
    "            entity_ok = entities_overlap(ent1, ent2)\n",
    "            threshold = 75 if \"how many\" in predictit_event.lower() or \"how many\" in polymarket_event.lower() or \"less than\" in predictit_event.lower() or \"less than\" in polymarket_event.lower() or \"between\" in predictit_event.lower() or \"between\" in polymarket_event.lower() or \">\" in predictit_event.lower() or \">\" in polymarket_event.lower() or \"<\" in predictit_event.lower() or \"<\" in polymarket_event.lower() else 75\n",
    "            if similarity >= threshold and timeframe_ok and entity_ok:\n",
    "                # Handle missing prices for arbitrage calculations\n",
    "                predictit_yes = predictit_subset['predictit_yes'].iloc[i]\n",
    "                predictit_no = predictit_subset['predictit_no'].iloc[i]\n",
    "                polymarket_yes = polymarket_subset['polymarket_yes'].iloc[j] * 100 if pd.notna(polymarket_subset['polymarket_yes'].iloc[j]) else None\n",
    "                polymarket_no = polymarket_subset['polymarket_no'].iloc[j] * 100 if pd.notna(polymarket_subset['polymarket_no'].iloc[j]) else None\n",
    "                \n",
    "                # Assume 100 - complementary price if missing\n",
    "                calc_predictit_yes = predictit_yes if pd.notna(predictit_yes) else (100 - predictit_no if pd.notna(predictit_no) else 0)\n",
    "                calc_predictit_no = predictit_no if pd.notna(predictit_no) else (100 - predictit_yes if pd.notna(predictit_yes) else 0)\n",
    "                calc_polymarket_yes = polymarket_yes if pd.notna(polymarket_yes) else (100 - polymarket_no if pd.notna(polymarket_no) else 0)\n",
    "                calc_polymarket_no = polymarket_no if pd.notna(polymarket_no) else (100 - polymarket_yes if pd.notna(polymarket_yes) else 0)\n",
    "                \n",
    "                matches.append({\n",
    "                    'PredictIt_Event': predictit_event,\n",
    "                    'Polymarket_Event': polymarket_event,\n",
    "                    'Similarity_Score': similarity,\n",
    "                    'Timeframes_Match': timeframe_ok,\n",
    "                    'Entities_Match': entity_ok,\n",
    "                    'PredictIt_Timeframes': tf1,\n",
    "                    'Polymarket_Timeframes': tf2,\n",
    "                    'PredictIt_Entities': ent1,\n",
    "                    'Polymarket_Entities': ent2,\n",
    "                    'predictit_yes': predictit_yes,\n",
    "                    'predictit_no': predictit_no,\n",
    "                    'polymarket_yes': polymarket_yes,\n",
    "                    'polymarket_no': polymarket_no,\n",
    "                    'arb1': 100 - calc_polymarket_yes - calc_predictit_no,\n",
    "                    'arb2': 100 - calc_polymarket_no - calc_predictit_yes\n",
    "                })\n",
    "    \n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    if not matches_df.empty:\n",
    "        matches_df = matches_df.sort_values(by='Similarity_Score', ascending=False)\n",
    "        matches_df = matches_df.drop_duplicates(subset=['PredictIt_Event', 'Polymarket_Event'], keep='first')\n",
    "    \n",
    "    # Filter for significant arbitrage and non-zero prices\n",
    "    matches_df = matches_df[\n",
    "        ((matches_df[\"arb1\"] >= 10) | (matches_df[\"arb2\"] >= 10)) &\n",
    "        (matches_df[\"predictit_yes\"].notna() & (matches_df[\"predictit_yes\"] != 0)) &\n",
    "        (matches_df[\"predictit_no\"].notna() & (matches_df[\"predictit_no\"] != 0)) &\n",
    "        (matches_df[\"polymarket_yes\"].notna() & (matches_df[\"polymarket_yes\"] != 0)) &\n",
    "        (matches_df[\"polymarket_no\"].notna() & (matches_df[\"polymarket_no\"] != 0))\n",
    "    ]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = r\"PredictIt_Polymarket_Matches.csv\" # replace with actual file location\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(matches_df)} matched markets\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    from rapidfuzz import fuzz\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    from tqdm.notebook import tqdm\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    \n",
    "    # Preprocess event names\n",
    "    def preprocess_name(name):\n",
    "        if pd.isna(name):\n",
    "            return \"\"\n",
    "        name = str(name).lower()\n",
    "        \n",
    "        # Synonym mapping\n",
    "        synonyms = {\n",
    "            'gubernatorial': 'governorship',\n",
    "            'presidential': 'president',\n",
    "            'senatorial': 'senate'\n",
    "        }\n",
    "        # State abbreviation mapping for all 50 states and DC\n",
    "        state_abbrevs = [\n",
    "            'al-', 'ak-', 'az-', 'ar-', 'ca-', 'co-', 'ct-', 'de-', 'fl-', 'ga-',\n",
    "            'hi-', 'id-', 'il-', 'in-', 'ia-', 'ks-', 'ky-', 'la-', 'me-', 'md-',\n",
    "            'ma-', 'mi-', 'mn-', 'ms-', 'mo-', 'mt-', 'ne-', 'nv-', 'nh-', 'nj-',\n",
    "            'nm-', 'ny-', 'nc-', 'nd-', 'oh-', 'ok-', 'or-', 'pa-', 'ri-', 'sc-',\n",
    "            'sd-', 'tn-', 'tx-', 'ut-', 'vt-', 'va-', 'wa-', 'wv-', 'wi-', 'wy-',\n",
    "            'dc-'\n",
    "        ]\n",
    "        state_names = [\n",
    "            'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'florida', 'georgia',\n",
    "            'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine', 'maryland',\n",
    "            'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska', 'nevada', 'new hampshire', 'new jersey',\n",
    "            'new mexico', 'new york', 'north carolina', 'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina',\n",
    "            'south dakota', 'tennessee', 'texas', 'utah', 'vermont', 'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming',\n",
    "            'district of columbia'\n",
    "        ]\n",
    "        state_dict = {abbrev[:-1]: name for abbrev, name in zip(state_abbrevs, state_names)}\n",
    "        for abbrev, full in state_dict.items():\n",
    "            name = re.sub(r'\\b' + re.escape(abbrev) + r'-(\\d+)\\b',\n",
    "                  lambda m: f\"{full} {m.group(1)} district\",\n",
    "                  name)\n",
    "        name = re.sub(r'\\b(\\d+)(st|nd|rd|th)\\b', r'\\1', name)\n",
    "        for old, new in synonyms.items():\n",
    "            name = re.sub(r'\\b' + old + r'\\b', new, name)\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        name = re.sub(r'district', 'district ', name)\n",
    "        return name\n",
    "    \n",
    "    # Extract timeframes\n",
    "    def extract_timeframe(text, end_date):\n",
    "        timeframes = []\n",
    "        text = str(text).lower()\n",
    "        years = re.findall(r'\\b(20\\d{2})\\b', text)\n",
    "        specific_dates = re.findall(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\s+\\d{1,2}\\s*,\\s*(20\\d{2})', text)\n",
    "        relative_terms = [\"this month\", \"this year\", \"before august\"] if \"before august\" in text else []\n",
    "        timeframes.extend(years)\n",
    "        timeframes.extend([date[0] for date in specific_dates])\n",
    "        timeframes.extend(relative_terms)\n",
    "        if isinstance(end_date, str) and not pd.isna(end_date):\n",
    "            try:\n",
    "                end_date_year = datetime.strptime(end_date, \"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "                timeframes.append(str(end_date_year))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        return list(set(timeframes))\n",
    "    \n",
    "    def extract_entities(text):\n",
    "        text = str(text)\n",
    "        # State abbreviation mapping for district normalization\n",
    "        pattern = r'\\b(?:[A-Z][a-z]*\\s+)+[A-Z][a-z]*\\b'\n",
    "        entities = re.findall(pattern, text)\n",
    "        district_pattern = r'\\b([A-Z][a-z\\s]*(?:\\'s)? \\d+(?:st|nd|rd|th)? District\\b|\\b[A-Z]{2}-\\d+\\b)'\n",
    "        districts = re.findall(district_pattern, text)\n",
    "        normalized_districts = []\n",
    "        # Include specific single-word entities and all state names (easily expandable to improve precision)\n",
    "        known_entities = {'US', 'Trump', 'Truth Social', 'The Witcher', 'Rotten Tomatoes', 'Republican', 'Democratic', 'Run', 'Run for', 'Win', 'Winner', 'Independent', 'Governor', 'Gubernatorial', 'Governorship', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming', 'District of Columbia'}\n",
    "        entities.extend([word for word in text.split() if word in known_entities])\n",
    "        numerical_patterns = [\n",
    "            r'\\b\\d+\\b(?!.*\\b20\\d{2}\\b)',\n",
    "            r'\\b\\d+-\\d+\\b',\n",
    "            r'\\b\\d+k\\b', \n",
    "            r'\\babove \\d+\\b', \n",
    "            r'\\bbelow \\d+\\b'\n",
    "        ]\n",
    "        for pattern in numerical_patterns:\n",
    "            matches = re.findall(pattern, text.lower())\n",
    "            entities.extend(matches)\n",
    "        # Normalize \"1k\" to \"1000\"\n",
    "        entities = [e.replace('1k', '1000') if '1k' in e else e for e in entities]\n",
    "        stop_words = {'Will', 'Who', 'Before', 'The', 'This', 'By', 'Be', 'On', 'At', 'Season', 'Cards', 'Day', 'How', 'Gold', 'Year', 'Above', 'Below', 'Score', 'Party'}\n",
    "        entities = [e.strip() for e in entities if e.strip() not in stop_words and len(e.strip()) > 2 and ':' not in e]\n",
    "        for entity in known_entities:\n",
    "            if entity.lower() in text.lower():\n",
    "                entities.append(entity)\n",
    "        return list(set(entities))\n",
    "    \n",
    "    # Check timeframe compatibility\n",
    "    def are_timeframes_compatible(tf1, tf2):\n",
    "        if not tf1 or not tf2:\n",
    "            return True\n",
    "        for t1 in tf1:\n",
    "            for t2 in tf2:\n",
    "                if t1 == t2:\n",
    "                    return True\n",
    "                if (t1 == \"this year\" or t1 == \"before august\") and t2.startswith(\"20\"):\n",
    "                    return True\n",
    "                if (t2 == \"this year\" or t2 == \"before august\") and t1.startswith(\"20\"):\n",
    "                    return True\n",
    "                if t1.startswith(\"20\") and t2.startswith(\"20\") and t1 == t2:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def extract_district(text):\n",
    "        text = text.lower()\n",
    "        match = re.search(r'(?:district|dist\\.?)\\s*(\\d+)|(\\d+)(?:st|nd|rd|th)?\\s*district', text)\n",
    "        if match:\n",
    "            return match.group(1) or match.group(2)\n",
    "        return None\n",
    "    \n",
    "    def same_district(name1, name2):\n",
    "        d1 = extract_district(name1)\n",
    "        d2 = extract_district(name2)\n",
    "        return d1 == d2  # True only if same district number or both None\n",
    "    \n",
    "    def entities_overlap(entities1, entities2):\n",
    "        if not entities1 or not entities2:\n",
    "            return True\n",
    "        # Define critical action words that must match or be absent\n",
    "        action_words = {'run', 'win', 'nominate', 'elect', 'appointed', 'confirmed'}\n",
    "        # Define party entities that must match or be absent\n",
    "        party_words = {'democratic', 'republican', 'independent'}\n",
    "    \n",
    "        \n",
    "        # Require matching state if both have one\n",
    "        us_states = {\n",
    "            'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut',\n",
    "            'delaware', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa',\n",
    "            'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan',\n",
    "            'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska', 'nevada',\n",
    "            'new hampshire', 'new jersey', 'new mexico', 'new york', 'north carolina',\n",
    "            'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island',\n",
    "            'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'vermont',\n",
    "            'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming', 'district of columbia'\n",
    "        }\n",
    "        \n",
    "        states1 = {e.lower() for e in entities1 if e.lower() in us_states}\n",
    "        states2 = {e.lower() for e in entities2 if e.lower() in us_states}\n",
    "    \n",
    "    \n",
    "        if len(states1) > 0 and len(states2) > 0:\n",
    "            for state1 in states1:\n",
    "                for state2 in states2:\n",
    "                    if state1 in state2:\n",
    "                        return True\n",
    "            for state2 in states2:\n",
    "                for state1 in states1:\n",
    "                    if state2 in state1:\n",
    "                        return True\n",
    "            return False\n",
    "    \n",
    "        \n",
    "        district_words = {e for e in entities1 if 'District' in e or '-' in e} | {e for e in entities2 if 'District' in e or '-' in e}\n",
    "        entities1_actions = [e.lower() for e in entities1 if e.lower() in action_words]\n",
    "        entities2_actions = [e.lower() for e in entities2 if e.lower() in action_words]\n",
    "        entities1_parties = [e.lower() for e in entities1 if e.lower() in party_words]\n",
    "        entities2_parties = [e.lower() for e in entities2 if e.lower() in party_words]\n",
    "        \n",
    "        # If either has an action word, they must match exactly\n",
    "        if entities1_actions or entities2_actions:\n",
    "            if set(entities1_actions) != set(entities2_actions):\n",
    "                return False\n",
    "    \n",
    "        # If either has a party, they must match exactly\n",
    "        if entities1_parties or entities2_parties:\n",
    "            if set(entities1_parties) != set(entities2_parties):\n",
    "                return False\n",
    "        \n",
    "        # Require at least one shared non-action, non-party, non-district entity\n",
    "        non_action_party_district_entities1 = [e for e in entities1 if e.lower() not in action_words and e.lower() not in party_words and e not in district_words]\n",
    "        non_action_party_district_entities2 = [e for e in entities2 if e.lower() not in action_words and e.lower() not in party_words and e not in district_words]\n",
    "        if not non_action_party_district_entities1 or not non_action_party_district_entities2:\n",
    "            return True\n",
    "        return any(e1.lower() in [e2.lower() for e2 in non_action_party_district_entities2] for e1 in non_action_party_district_entities1)\n",
    "    \n",
    "    kalshi_data = pd.read_csv(r\"Kalshi.csv\") # replace with actual file location\n",
    "        \n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"AZ-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bAZ-', 'Arizona District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"CA-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bCA-', 'California District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"CT-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bCT-', 'Connecticut District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"CO-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bCO-', 'Colorado District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"IL-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bIL-', 'Illinois District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"IN-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bIN-', 'Indiana District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"IA-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bIA-', 'Iowa District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"ME-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bME-', 'Maine District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"MI-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bMI-', 'Michigan District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"MN-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bMN-', 'Minnesota District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"MT-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bMT-', 'Montana District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"NC-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bNC-', 'North Carolina District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"NE-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bNE-', 'Nebraska District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"NH-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bNH-', 'New Hampshire District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"NJ-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bNJ-', 'New Jersey District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"NM-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bNM-', 'New Mexico District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"NV-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bNV-', 'Nevada District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"NY-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bNY-', 'New York District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"OH-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bOH-', 'Ohio District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"OR-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bOR-', 'Oregon District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"PA-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bPA-', 'Pennsylvania District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"TN-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bTN-', 'Tennessee District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"VA-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bVA-', 'Virginia District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"WA-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bWA-', 'Washington District', regex=True)\n",
    "    kalshi_data.loc[kalshi_data['full_name'].str.contains(\"WI-\", na=False), 'full_name'] = \\\n",
    "        kalshi_data['full_name'].str.replace(r'\\bWI-', 'Wisconsin District', regex=True)\n",
    "\n",
    "    predictit_data = pd.read_csv(r\"C:\\Users\\johnt\\Downloads\\Prediction Markets\\PredictIt.csv\")\n",
    "    predictit_data['price'] = predictit_data['price'] * 100\n",
    "    \n",
    "    # Pivot data for prices\n",
    "    kalshi_pivot = kalshi_data.pivot_table(\n",
    "        index=[\"full_name\", \"market_id\", \"end_date\"],\n",
    "        columns=\"side\",\n",
    "        values=\"ask\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "    kalshi_pivot.columns.name = None\n",
    "    kalshi_pivot.rename(columns={\"YES\": \"kalshi_yes\", \"NO\": \"kalshi_no\"}, inplace=True)\n",
    "    \n",
    "    predictit_pivot = predictit_data.pivot_table(\n",
    "        index=[\"market_name\", \"market_id\", \"end_date\"],\n",
    "        columns=\"side\",\n",
    "        values=\"price\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "    predictit_pivot.columns.name = None\n",
    "    predictit_pivot.rename(columns={\"YES\": \"predictit_yes\", \"NO\": \"predictit_no\"}, inplace=True)\n",
    "    \n",
    "    # Preprocess event names and extract timeframes\n",
    "    kalshi_pivot['processed_name'] = kalshi_pivot['full_name'].apply(preprocess_name)\n",
    "    predictit_pivot['processed_name'] = predictit_pivot['market_name'].apply(preprocess_name)\n",
    "    kalshi_pivot['timeframes'] = kalshi_pivot.apply(lambda x: extract_timeframe(x['full_name'], x['end_date']), axis=1)\n",
    "    predictit_pivot['timeframes'] = predictit_pivot.apply(lambda x: extract_timeframe(x['market_name'], x['end_date']), axis=1)\n",
    "    kalshi_pivot['entities'] = kalshi_pivot['full_name'].apply(extract_entities)\n",
    "    predictit_pivot['entities'] = predictit_pivot['market_name'].apply(extract_entities)\n",
    "    kalshi_subset = kalshi_pivot\n",
    "    predictit_subset = predictit_pivot\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    kalshi_vectors = vectorizer.fit_transform(kalshi_subset['processed_name'])\n",
    "    predictit_vectors = vectorizer.transform(predictit_subset['processed_name'])\n",
    "    cosine_sim = cosine_similarity(kalshi_vectors, predictit_vectors)\n",
    "    \n",
    "    # Filter pairs with high similarity\n",
    "    cosine_threshold = 0.72\n",
    "    similar_pairs = np.where(cosine_sim >= cosine_threshold)\n",
    "    matches = []\n",
    "    \n",
    "    # Higher precision matching\n",
    "    for idx in tqdm(range(len(similar_pairs[0])), desc=\"Matching similar pairs\"):\n",
    "        i, j = similar_pairs[0][idx], similar_pairs[1][idx]\n",
    "        name1 = kalshi_subset['processed_name'].iloc[i]\n",
    "        name2 = predictit_subset['processed_name'].iloc[j]\n",
    "        tf1 = kalshi_subset['timeframes'].iloc[i]\n",
    "        tf2 = predictit_subset['timeframes'].iloc[j]\n",
    "        ent1 = kalshi_subset['entities'].iloc[i]\n",
    "        ent2 = predictit_subset['entities'].iloc[j]\n",
    "        kalshi_event = kalshi_subset['full_name'].iloc[i]\n",
    "        predictit_event = predictit_subset['market_name'].iloc[j]\n",
    "        if name1 and name2:\n",
    "            similarity = fuzz.token_sort_ratio(name1, name2)\n",
    "            timeframe_ok = are_timeframes_compatible(tf1, tf2)\n",
    "            entity_ok = entities_overlap(ent1, ent2)\n",
    "            kalshi_event = re.sub(r'District', 'District ', kalshi_event)\n",
    "            threshold = 80 if \"how many\" in kalshi_event.lower() or \"how many\" in predictit_event.lower() or \"less than\" in kalshi_event.lower() or \"less than\" in predictit_event.lower() or \"between\" in kalshi_event.lower() or \"between\" in predictit_event.lower() or \">\" in kalshi_event.lower() or \">\" in predictit_event.lower() or \"<\" in kalshi_event.lower() or \"<\" in predictit_event.lower() else 80\n",
    "            if similarity >= threshold and timeframe_ok and entity_ok and same_district(name1, name2):\n",
    "                # Handle missing prices for arbitrage calculations\n",
    "                kalshi_yes = kalshi_subset['kalshi_yes'].iloc[i]\n",
    "                kalshi_no = kalshi_subset['kalshi_no'].iloc[i]\n",
    "                predictit_yes = predictit_subset['predictit_yes'].iloc[j]\n",
    "                predictit_no = predictit_subset['predictit_no'].iloc[j]\n",
    "                \n",
    "                # Assume 100 - complementary price if missing\n",
    "                calc_kalshi_yes = kalshi_yes if pd.notna(kalshi_yes) else (100 - kalshi_no if pd.notna(kalshi_no) else 0)\n",
    "                calc_kalshi_no = kalshi_no if pd.notna(kalshi_no) else (100 - kalshi_yes if pd.notna(kalshi_yes) else 0)\n",
    "                calc_predictit_yes = predictit_yes if pd.notna(predictit_yes) else (100 - predictit_no if pd.notna(predictit_no) else 0)\n",
    "                calc_predictit_no = predictit_no if pd.notna(predictit_no) else (100 - predictit_yes if pd.notna(predictit_yes) else 0)\n",
    "                \n",
    "                matches.append({\n",
    "                    'Kalshi_Event': kalshi_event,\n",
    "                    'PredictIt_Event': predictit_event,\n",
    "                    'Similarity_Score': similarity,\n",
    "                    'Timeframes_Match': timeframe_ok,\n",
    "                    'Entities_Match': entity_ok,\n",
    "                    'Kalshi_Timeframes': tf1,\n",
    "                    'PredictIt_Timeframes': tf2,\n",
    "                    'Kalshi_Entities': ent1,\n",
    "                    'PredictIt_Entities': ent2,\n",
    "                    'kalshi_yes': kalshi_yes,\n",
    "                    'kalshi_no': kalshi_no,\n",
    "                    'predictit_yes': predictit_yes,\n",
    "                    'predictit_no': predictit_no,\n",
    "                    'arb1': 100 - calc_predictit_yes - calc_kalshi_no,\n",
    "                    'arb2': 100 - calc_predictit_no - calc_kalshi_yes\n",
    "                })\n",
    "    \n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    if not matches_df.empty:\n",
    "        matches_df = matches_df.sort_values(by='Similarity_Score', ascending=False)\n",
    "        matches_df = matches_df.drop_duplicates(subset=['Kalshi_Event', 'PredictIt_Event'], keep='first')\n",
    "    \n",
    "    \n",
    "    # Filter for significant arbitrage and non-zero prices\n",
    "    matches_df = matches_df[\n",
    "        ((matches_df[\"arb1\"] >= 5) | (matches_df[\"arb2\"] >= 5)) &\n",
    "        (matches_df[\"kalshi_yes\"].notna() & (matches_df[\"kalshi_yes\"] != 0)) &\n",
    "        (matches_df[\"kalshi_no\"].notna() & (matches_df[\"kalshi_no\"] != 0)) &\n",
    "        (matches_df[\"predictit_yes\"].notna() & (matches_df[\"predictit_yes\"] != 0)) &\n",
    "        (matches_df[\"predictit_no\"].notna() & (matches_df[\"predictit_no\"] != 0))\n",
    "    ]\n",
    "    \n",
    "    # Filter out mismatched \"west virginia\" presence (common occurrence)\n",
    "    matches_df = matches_df[\n",
    "        matches_df.apply(\n",
    "            lambda row: (\"west virginia\" in row[\"Kalshi_Event\"].lower()) == (\"west virginia\" in row[\"PredictIt_Event\"].lower()),\n",
    "            axis=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    output_path = r\"Kalshi_PredictIt_Matches.csv\" # replace with desired file location\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {len(matches_df)} matched markets\")\n",
    "        \n",
    "    POST_WEBHOOK_URL = \"https://discord.com/api/webhooks/\"  # Replace with 'post' channel webhook\n",
    "    SEND_WEBHOOK_URL = \"https://discord.com/api/webhooks/\"  # Replace with 'send' channel webhook\n",
    "    \n",
    "    # Webhook helper function\n",
    "    def send_webhook_message(webhook_url, content, username=None, avatar_url=None):\n",
    "        # Sends a message to a Discord channel using a webhook.\n",
    "        payload = {\n",
    "            \"content\": content,\n",
    "            \"username\": username,\n",
    "            \"avatar_url\": avatar_url\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(webhook_url, json=payload)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to send message: {e}\")\n",
    "    \n",
    "    def post(msg, fileName):\n",
    "        # Sends a low-priority notification to the designated 'post' channel using a webhook.\n",
    "        if \"Kalshi_PredictIt_Matches_New_Or_Changed\" in fileName:\n",
    "            POST_WEBHOOK_URL = \"https://discord.com/api/webhooks/\" # replace with actual post webhook url\n",
    "            path = pd.read_csv(r\"Kalshi_PredictIt_Matches_New_Or_Changed.csv\") # replace with desired path\n",
    "            for i in range(len(path)):\n",
    "                if path.shape[1] > 15:\n",
    "                    row = path.iloc[i, [0, 1, 13, 14, 15, 16]]\n",
    "                else:\n",
    "                    row = path.iloc[i, [0, 1, 13, 14]]\n",
    "                if pd.notna(row[3]) and isinstance(row[3], (int, float)):\n",
    "                    pass\n",
    "                else:\n",
    "                    row[2] = row[4]\n",
    "                    row[3] = row[5]\n",
    "                msg = (\n",
    "                    f\"Kalshi Event: {row[0]}\\n\"\n",
    "                    f\"PredictIt Event: {row[1]}\\n\"\n",
    "                    f\"Arb1: {row[2]:.2f}\\n\"\n",
    "                    f\"Arb2: {row[3]:.2f}\"\n",
    "                )\n",
    "                title = \"\"\n",
    "                content = f\"{title}\\n{msg}\"\n",
    "                send_webhook_message(POST_WEBHOOK_URL, content, username=\"Projections Bot\")\n",
    "                \n",
    "        elif \"PredictIt_Polymarket_Matches_New_Or_Changed\" in fileName:\n",
    "            POST_WEBHOOK_URL = 'https://discord.com/api/webhooks/' # replace with actual post webhook url\n",
    "            path = pd.read_csv(r\"PredictIt_Polymarket_Matches_New_Or_Changed.csv\") # replace with desired path\n",
    "            for i in range(len(path)):\n",
    "                if path.shape[1] > 15:\n",
    "                    row = path.iloc[i, [0, 1, 13, 14, 15, 16]]\n",
    "                else:\n",
    "                    row = path.iloc[i, [0, 1, 13, 14]]\n",
    "                if pd.notna(row[3]) and isinstance(row[3], (int, float)):\n",
    "                    pass\n",
    "                else:\n",
    "                    row[2] = row[4]\n",
    "                    row[3] = row[5]\n",
    "                msg = (\n",
    "                    f\"PredictIt Event: {row[0]}\\n\"\n",
    "                    f\"Polymarket Event: {row[1]}\\n\"\n",
    "                    f\"Arb1: {row[2]:.2f}\\n\"\n",
    "                    f\"Arb2: {row[3]:.2f}\"\n",
    "                )\n",
    "                title = \"\"\n",
    "                content = f\"{title}\\n{msg}\"\n",
    "                send_webhook_message(POST_WEBHOOK_URL, content, username=\"Projections Bot\")\n",
    "                \n",
    "        elif \"Kalshi_Polymarket_Matches_New_Or_Changed\" in fileName:\n",
    "            POST_WEBHOOK_URL = 'https://discord.com/api/webhooks/' # replace with actual post webhook url\n",
    "            path = pd.read_csv(r\"Kalshi_Polymarket_Matches_New_Or_Changed.csv\") # replace with desired path\n",
    "            for i in range(len(path)):\n",
    "                if path.shape[1] > 15:\n",
    "                    row = path.iloc[i, [0, 1, 13, 14, 15, 16]]\n",
    "                else:\n",
    "                    row = path.iloc[i, [0, 1, 13, 14]]\n",
    "                if pd.notna(row[3]) and isinstance(row[3], (int, float)):\n",
    "                    pass\n",
    "                else:\n",
    "                    row[2] = row[4]\n",
    "                    row[3] = row[5]\n",
    "                msg = (\n",
    "                    f\"Kalshi Event: {row[0]}\\n\"\n",
    "                    f\"Polymarket Event: {row[1]}\\n\"\n",
    "                    f\"Arb1: {row[2]:.2f}\\n\"\n",
    "                    f\"Arb2: {row[3]:.2f}\"\n",
    "                )\n",
    "                title = \"\"\n",
    "                content = f\"{title}\\n{msg}\"\n",
    "                send_webhook_message(POST_WEBHOOK_URL, content, username=\"Projections Bot\")\n",
    "    \n",
    "    # Compare matches and identify new or changed events\n",
    "    def compare_matches(new_matches_df, old_file_path, output_path, platform1_name, platform2_name, new_name):\n",
    "        new_matches_df['event_pair'] = new_matches_df[f'{platform1_name}_Event'] + '|' + new_matches_df[f'{platform2_name}_Event']        \n",
    "        if os.path.exists(old_file_path):\n",
    "            old_matches_df = pd.read_csv(old_file_path)\n",
    "            old_matches_df['event_pair'] = old_matches_df[f'{platform1_name}_Event'] + '|' + old_matches_df[f'{platform2_name}_Event']\n",
    "            \n",
    "            # Find new events\n",
    "            new_events = new_matches_df[~new_matches_df['event_pair'].isin(old_matches_df['event_pair'])]\n",
    "            \n",
    "            # Find events with increased arbitrage value (>= 5)\n",
    "            common_events = new_matches_df[new_matches_df['event_pair'].isin(old_matches_df['event_pair'])]\n",
    "            if not common_events.empty:\n",
    "                common_events = common_events.merge(\n",
    "                    old_matches_df[['event_pair', 'arb1', 'arb2']],\n",
    "                    on='event_pair',\n",
    "                    suffixes=('_new', '_old')\n",
    "                )\n",
    "                changed_events = common_events[\n",
    "                    (common_events['arb1_new'] >= common_events['arb1_old'] + 15) |\n",
    "                    (common_events['arb2_new'] >= common_events['arb2_old'] + 15)\n",
    "                ]\n",
    "            else:\n",
    "                changed_events = pd.DataFrame()\n",
    "            \n",
    "            # Combine new and changed events\n",
    "            new_or_changed_df = pd.concat([new_events, changed_events], ignore_index=True)\n",
    "            \n",
    "            if not new_or_changed_df.empty:\n",
    "                new_or_changed_df = new_or_changed_df.drop(columns=['event_pair', 'arb1_old', 'arb2_old'], errors='ignore')\n",
    "                new_or_changed_df.to_csv(output_path, index=False)\n",
    "                print(f\"Saved {len(new_or_changed_df)} new or significantly changed events to {output_path}\")\n",
    "                post(\"test1\", new_name)\n",
    "            else:\n",
    "                print(f\"No new or significantly changed events found for {platform1_name}-{platform2_name}\")\n",
    "        else:\n",
    "            # If no old file exists, all events are new\n",
    "            new_matches_df.drop(columns=['event_pair'], errors='ignore').to_csv(output_path, index=False)\n",
    "            print(f\"Saved {len(new_matches_df)} new events to {output_path} (no old file found)\")\n",
    "            post(\"test1\", new_name)\n",
    "    \n",
    "    # Kalshi-PredictIt comparison\n",
    "    matches_df = pd.read_csv(r\"Kalshi_PredictIt_Matches.csv\") # replace with actual file location\n",
    "    old_kalshi_predictit_file = r\"Kalshi_PredictIt_Matches_Old.csv\" # replace with actual file location\n",
    "    new_or_changed_kalshi_predictit_path = r\"Kalshi_PredictIt_Matches_New_Or_Changed.csv\" # replace with actual file location\n",
    "    compare_matches(matches_df, old_kalshi_predictit_file, new_or_changed_kalshi_predictit_path, 'Kalshi', 'PredictIt', \"Kalshi_PredictIt_Matches_New_Or_Changed\")\n",
    "    \n",
    "    \n",
    "    # Compare Kalshi-Polymarket matches\n",
    "    matches_kp_df = pd.read_csv(r\"Kalshi_Polymarket_Matches.csv\") # replace with actual file location\n",
    "    old_kalshi_polymarket_file = r\"Kalshi_Polymarket_Matches_Old.csv\" # replace with actual file location\n",
    "    new_or_changed_kalshi_polymarket_path = r\"Kalshi_Polymarket_Matches_New_Or_Changed.csv\" # replace with actual file location\n",
    "    compare_matches(matches_kp_df, old_kalshi_polymarket_file, new_or_changed_kalshi_polymarket_path, 'Kalshi', 'Polymarket', \"Kalshi_Polymarket_Matches_New_Or_Changed.csv\")\n",
    "    \n",
    "    \n",
    "    # Compare PredictIt-Polymarket matches\n",
    "    matches_pp_df = pd.read_csv(r\"PredictIt_Polymarket_Matches.csv\") # replace with actual file location\n",
    "    old_predictit_polymarket_file = r\"PredictIt_Polymarket_Matches_Old.csv\" # replace with actual file location\n",
    "    new_or_changed_predictit_polymarket_path = r\"PredictIt_Polymarket_Matches_New_Or_Changed.csv\" # replace with actual file location\n",
    "    compare_matches(matches_pp_df, old_predictit_polymarket_file, new_or_changed_predictit_polymarket_path, 'PredictIt', 'Polymarket', \"PredictIt_Polymarket_Matches_New_Or_Changed.csv\")\n",
    "\n",
    "while True:\n",
    "    import time\n",
    "    prediction_markets()\n",
    "    time.sleep(3600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
